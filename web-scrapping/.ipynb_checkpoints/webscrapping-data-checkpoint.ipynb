{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896fa5f4-0d32-4012-ae8a-41ffbc8e2cc5",
   "metadata": {},
   "source": [
    "## Web Scrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa155648-9fba-4bff-8bba-fcb1dc0867c5",
   "metadata": {},
   "source": [
    "##### Whenever a website from which we need data isn't providing api,the only option left to get that data is web scrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45eea67-f8d0-4c98-b39f-0285d01f5a24",
   "metadata": {},
   "source": [
    "##### For web-scrapping , we need  BeautifulSoup library form the bs4 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494f6a66-4535-4693-8090-0efb275ac925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8efff-5dcb-4eb6-a424-fc97132461a3",
   "metadata": {},
   "source": [
    "##### In the case of some websites ,when we try to web scrap data from their website , access is denied from them as they dont want any bot like us to scrap their data .In such cases ,we get repsonse 403 which means permission is denied .We can have a look to it -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5863cc8-cbb4-4547-b78a-500e0ff25d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://www.ambitionbox.com/list-of-companies?page=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4a97b-779c-417d-9ea9-f407fbad6c86",
   "metadata": {},
   "source": [
    "##### We can actually get more info about this error using text attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac75cff-3788-42fe-9937-89059902989d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<HTML><HEAD>\\n<TITLE>Access Denied</TITLE>\\n</HEAD><BODY>\\n<H1>Access Denied</H1>\\n \\nYou don\\'t have permission to access \"http&#58;&#47;&#47;www&#46;ambitionbox&#46;com&#47;list&#45;of&#45;companies&#63;\" on this server.<P>\\nReference&#32;&#35;18&#46;7efed417&#46;1702031724&#46;15d6e6be\\n</BODY>\\n</HTML>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://www.ambitionbox.com/list-of-companies?page=1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d302f82-3522-4195-b374-f6cc4a09b9a4",
   "metadata": {},
   "source": [
    "##### In such cases ,we use a parameter called headers to make the webiste believe that request is made by any human via a browser.So in this case,it grants us acces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14a23da-588b-48e7-a8ea-cd47633a29cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ebb2aba-c2c2-45a8-aa72-4d89e00cd8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "webpage=requests.get('https://www.ambitionbox.com/list-of-companies?page=1',headers=headers).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64964b-411e-4be8-852e-4b608fa728ec",
   "metadata": {},
   "source": [
    "##### So ,in above line we have stored all the html cotnet of webpage inside the webpage variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead755c-3a2e-4db2-bf50-cd0bbd2ebe0f",
   "metadata": {},
   "source": [
    "##### Now beautifulsoup is used to extract the data that we want from the fetched hmtl content. We use BeautifulSoup() which takes 2 arguments ,contents of webpage i.e in our case is stored in webpage and second is html parser format which is 'lxml'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af124e17-5ead-4f5f-aa2e-8d2a075ac702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(webpage,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0691a-85ad-42ca-b111-5d3a61c0b60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
