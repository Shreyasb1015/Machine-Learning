{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896fa5f4-0d32-4012-ae8a-41ffbc8e2cc5",
   "metadata": {},
   "source": [
    "## Web Scrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa155648-9fba-4bff-8bba-fcb1dc0867c5",
   "metadata": {},
   "source": [
    "##### Whenever a website from which we need data isn't providing api,the only option left to get that data is web scrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45eea67-f8d0-4c98-b39f-0285d01f5a24",
   "metadata": {},
   "source": [
    "##### For web-scrapping , we need  BeautifulSoup library form the bs4 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494f6a66-4535-4693-8090-0efb275ac925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8efff-5dcb-4eb6-a424-fc97132461a3",
   "metadata": {},
   "source": [
    "##### In the case of some websites ,when we try to web scrap data from their website , access is denied from them as they dont want any bot like us to scrap their data .In such cases ,we get repsonse 403 which means permission is denied .We can have a look to it -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5863cc8-cbb4-4547-b78a-500e0ff25d32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://www.ambitionbox.com/list-of-companies?page=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4a97b-779c-417d-9ea9-f407fbad6c86",
   "metadata": {},
   "source": [
    "##### We can actually get more info about this error using text attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac75cff-3788-42fe-9937-89059902989d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<HTML><HEAD>\\n<TITLE>Access Denied</TITLE>\\n</HEAD><BODY>\\n<H1>Access Denied</H1>\\n \\nYou don\\'t have permission to access \"http&#58;&#47;&#47;www&#46;ambitionbox&#46;com&#47;list&#45;of&#45;companies&#63;\" on this server.<P>\\nReference&#32;&#35;18&#46;7efed417&#46;1702031724&#46;15d6e6be\\n</BODY>\\n</HTML>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://www.ambitionbox.com/list-of-companies?page=1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d302f82-3522-4195-b374-f6cc4a09b9a4",
   "metadata": {},
   "source": [
    "##### In such cases ,we use a parameter called headers to make the webiste believe that request is made by any human via a browser.So in this case,it grants us acces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14a23da-588b-48e7-a8ea-cd47633a29cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ebb2aba-c2c2-45a8-aa72-4d89e00cd8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "webpage=requests.get('https://www.ambitionbox.com/list-of-companies?page=1',headers=headers).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64964b-411e-4be8-852e-4b608fa728ec",
   "metadata": {},
   "source": [
    "##### So ,in above line we have stored all the html cotnet of webpage inside the webpage variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead755c-3a2e-4db2-bf50-cd0bbd2ebe0f",
   "metadata": {},
   "source": [
    "##### Now beautifulsoup is used to extract the data that we want from the fetched hmtl content. We use BeautifulSoup() which takes 2 arguments ,contents of webpage i.e in our case is stored in webpage and second is html parser format which is 'lxml'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af124e17-5ead-4f5f-aa2e-8d2a075ac702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(webpage, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58604c-8378-429b-b0cb-fa371f318ced",
   "metadata": {},
   "source": [
    "##### Now we can access the BeautifulSoup class functions and attributes as we have create aobject of it called as soup.We have prettify() which is used to make our html content look pretty i.e in nice order .You should print it ,so as it to see its output.This step helps us in understanding the structure of webpge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "847beb46-f2fa-4f23-80c0-df6fbd8b2329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c5a32-6b77-46da-bb83-fac07ff2d348",
   "metadata": {},
   "source": [
    "##### We use find_all() to find any div/tags or anythin.For e.g if we want to find all the h1 tags on the webpage,we can pass it as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5649e060-5789-40ee-9be3-b4e016192b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"companyListing__title\">\n",
       " \t\t\t\t\t\t\tList of companies in India\n",
       " \t\t\t\t\t\t</h1>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9d49e-de81-4083-9630-a3793347e72f",
   "metadata": {},
   "source": [
    "##### As we can see ,there is only one h1 tag element.It is stored in list.To access the text inside this element .we can use [ ] as we are working with list and select 0 as list only has one element and we want to fetch first elements detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eedcdabd-76a3-4fba-9cc7-74b3f244adb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"companyListing__title\">\n",
       "\t\t\t\t\t\t\tList of companies in India\n",
       "\t\t\t\t\t\t</h1>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4a5b8-3175-473e-a3bd-c15b13272e68",
   "metadata": {},
   "source": [
    "##### Now ,to extract the text ,we can use text attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d54bf40-d08e-43df-8818-2a8f29ef14bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\t\\t\\t\\t\\t\\t\\tList of companies in India\\n\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h1')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58e58c5-1302-4b82-bee3-08895421d269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 class=\"companyCardWrapper__companyName\" title=\"TCS\">\n",
       " \t\t\t\t\t\t\t\t\t\tTCS\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Accenture\">\n",
       " \t\t\t\t\t\t\t\t\t\tAccenture\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Cognizant\">\n",
       " \t\t\t\t\t\t\t\t\t\tCognizant\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Wipro\">\n",
       " \t\t\t\t\t\t\t\t\t\tWipro\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"HDFC Bank\">\n",
       " \t\t\t\t\t\t\t\t\t\tHDFC Bank\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"ICICI Bank\">\n",
       " \t\t\t\t\t\t\t\t\t\tICICI Bank\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Infosys\">\n",
       " \t\t\t\t\t\t\t\t\t\tInfosys\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Capgemini\">\n",
       " \t\t\t\t\t\t\t\t\t\tCapgemini\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"HCLTech\">\n",
       " \t\t\t\t\t\t\t\t\t\tHCLTech\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Tech Mahindra\">\n",
       " \t\t\t\t\t\t\t\t\t\tTech Mahindra\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Genpact\">\n",
       " \t\t\t\t\t\t\t\t\t\tGenpact\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Axis Bank\">\n",
       " \t\t\t\t\t\t\t\t\t\tAxis Bank\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Concentrix Corporation\">\n",
       " \t\t\t\t\t\t\t\t\t\tConcentrix Corporation\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Amazon\">\n",
       " \t\t\t\t\t\t\t\t\t\tAmazon\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Teleperformance\">\n",
       " \t\t\t\t\t\t\t\t\t\tTeleperformance\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Reliance Jio\">\n",
       " \t\t\t\t\t\t\t\t\t\tReliance Jio\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"IBM\">\n",
       " \t\t\t\t\t\t\t\t\t\tIBM\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Larsen &amp; Toubro Limited\">\n",
       " \t\t\t\t\t\t\t\t\t\tLarsen &amp; Toubro Limited\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"Reliance Retail\">\n",
       " \t\t\t\t\t\t\t\t\t\tReliance Retail\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"companyCardWrapper__companyName\" title=\"HDB Financial Services\">\n",
       " \t\t\t\t\t\t\t\t\t\tHDB Financial Services\n",
       " \t\t\t\t\t\t\t\t\t</h2>,\n",
       " <h2 class=\"main-title body-medium\">\n",
       " \t\t\tCompanies by  Industry\n",
       " \t\t</h2>,\n",
       " <h2 class=\"main-title body-medium\">\n",
       " \t\t\tCompanies by  Locations\n",
       " \t\t</h2>,\n",
       " <h2 class=\"main-title body-medium\">\n",
       " \t\t\tCompanies by  Type\n",
       " \t\t</h2>,\n",
       " <h2 class=\"main-title body-medium\">\n",
       " \t\t\tCompanies by  Badges\n",
       " \t\t</h2>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fa772-e360-433e-8935-2210d2fde1fb",
   "metadata": {},
   "source": [
    "##### As we can see ,there are more no og h2 tags on the webpage.We can also use len() to calculate the total elements ,as len() is used to calculate length of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef2e5069-d90b-4dfe-9d2e-dfd9577ceef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('h2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fdbdb-0120-40ee-9c61-0a7508701b70",
   "metadata": {},
   "source": [
    "##### To print all the comapny's names ,we can use for loop and to remove the whitespace characters we can use strip()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ce265d0-9ed6-4931-9268-dd10fd70acd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCS\n",
      "Accenture\n",
      "Cognizant\n",
      "Wipro\n",
      "HDFC Bank\n",
      "ICICI Bank\n",
      "Infosys\n",
      "Capgemini\n",
      "HCLTech\n",
      "Tech Mahindra\n",
      "Genpact\n",
      "Axis Bank\n",
      "Concentrix Corporation\n",
      "Amazon\n",
      "Teleperformance\n",
      "Reliance Jio\n",
      "IBM\n",
      "Larsen & Toubro Limited\n",
      "Reliance Retail\n",
      "HDB Financial Services\n",
      "Companies by  Industry\n",
      "Companies by  Locations\n",
      "Companies by  Type\n",
      "Companies by  Badges\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('h2'):\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cadd1e-1523-4315-a084-9cb24ca1fa38",
   "metadata": {},
   "source": [
    "##### As we can see ,last three rows are not names of companies.So we can remove them using remove()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bc51fae-ab4f-434f-8566-e43420a3a3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list=soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a77a15-954a-401e-9859-4dad57504740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list=list[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3063bd5-1307-4418-8629-f06d8d488c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCS\n",
      "Accenture\n",
      "Cognizant\n",
      "Wipro\n",
      "HDFC Bank\n",
      "ICICI Bank\n",
      "Infosys\n",
      "Capgemini\n",
      "HCLTech\n",
      "Tech Mahindra\n",
      "Genpact\n",
      "Axis Bank\n",
      "Concentrix Corporation\n",
      "Amazon\n",
      "Teleperformance\n",
      "Reliance Jio\n",
      "IBM\n"
     ]
    }
   ],
   "source": [
    "for i in list:\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b0d36-08fe-410a-9858-af2d834243dc",
   "metadata": {},
   "source": [
    "##### Now to extract the ratings of each company ,we need to extract the spans in html content.But there exists some spans which are showcasing some other info .To differentiate with such elements ,we can use the class of elements which is specified while bulding website for each element.We need to pass class argument which is set to the class name of the element which we want to scrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7baa25a-c4fb-4c4b-bf0a-c327e280a73e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"companyCardWrapper__companyRatingValue\">3.8</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">4.0</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.8</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">4.0</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.8</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.7</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.7</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.8</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">4.1</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.6</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">4.1</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">4.0</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">3.9</span>,\n",
       " <span class=\"companyCardWrapper__companyRatingValue\">4.0</span>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('span',class_='companyCardWrapper__companyRatingValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ddfd57b-dac0-46f3-9f32-4094fd6551c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('span',class_='companyCardWrapper__companyRatingValue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814df939-dfa6-4028-8a86-5a4a7c3808fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
